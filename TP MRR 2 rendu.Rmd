---
title: "Tp MRR 2"
author: "Mame Diarra Toure"
date: "10/14/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\color{magenta}\section{I.Study of the UScrime dataset}
\color{black}
As data Scientists we are asked to study the ability of regular linear models, lasso or ridge to predict the US crime rate dataset.
\color{red}\subsection{1.Preliminary work}
\color{black}
First of all we start with uploading the data and computing the correlation matrix to have a first sight on our dataset.

```{r,include=FALSE}
library(corrplot)
library(MASS)
```
\tiny
```{r,echo=FALSE}
data(UScrime)
corrplot(cor(UScrime), method="circle")
```
\normalsize
\newline The correlation plot shows us that our target variable is not correlated to all the covariables, hence a linear model containing all the 15 covariable may not be the best choice here.
We also notice that some of the co-variables are strongly correlated to each other (Po1 and GDP for exemple),thus they shouldn't be both on the model. 
So we are going to do  variable selection(forward,backward,stepwise) and then study the penalized regressions model (Lasso and ridge) and then decide which of those models are the best from a predictive point of view. 

\color{magenta}\section{Selection variables }
\color{red}\subsection{forward selection}
\color{black}
Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one. In each forward step, you add the one variable that gives the single best improvement to your model.
In the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters fit; at each step an add or drop will be performed that minimizes the AIC score.
\tiny
```{r,include=FALSE}
library(MASS)
library(glmnet)

crimereg <- lm(y~.,data = UScrime)
```

```{r}
regforward=step(lm(UScrime$y~1,data=UScrime),list(upper=crimereg),direction = 'forward')
```
The forward selection add successively Po1 , Ineq , Ed , M , Prob , U2
\color{red}\subsection{Backward selection}
\color{black}
Backward selection is a type of stepwise regression which begins  with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.
In the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters fit; at each step a drop will be performed that minimizes the AIC score.
```{r}
regbackward<-step(crimereg,direction = "backward")
```
There are 8 variables  that are selected. The backward selection drops successively So, time, lf, nw, po2, pop, gdp
\color{red}\subsection{Stepwise selection}
\color{black}
Stepwise regression is a combination of the forward and backward selection techniques. It is a modification of the forward selection so that after each step in which a variable was added, all candidate variables in the model are checked to see if their significance has been reduced below the specified tolerance level. If a nonsignificant variable is found, it is removed from the model.

```{r}
regboth<-step(crimereg,direction = 'both')

```
\color{magenta}\section{Penalized regression models }
By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process, variables are either retained or discarded. It often exhibits high variance, and so doesn’t reduce the prediction error of the full model. Shrinkage methods are more continuous, and don’t suffer as much from high variability.
\color{red}\subsection{Lasso regression}
\color{black}
Lasso regression find a parsimonious model which performs L1 regularization. The L1 regularization adds a penality equivalent to the absolute of the maginitude of regression coefficients and tries to minimize them 
Lasso translates each coefficient by a constant factor $\lambda$, truncating at zero. This is called “soft thresholding”.
```{r}
X<- as.matrix(UScrime[ ,1:15])
Y <-as.matrix(UScrime$y)
LassoMod <- glmnet(X, Y, alpha=1, nlambda=100,lambda.min.ratio=0.0001)
```
Let us now see how the coefficient values change according to the lambda value. To do that, we will use the plot.glmnet()nction that produces a coefficient profile plot of the coefficient paths for a fitted glmnet obeject
```{r}
 plot(LassoMod,xvar="norm",label=TRUE)
```
In the previous figure, each curve corresponds to a variable. This plot shows the path of its coefficient against the L1-norm of the whole coefficient vector when $\lambda$ varying. The previous axis indicates the number of nonzero coefficients at the current $\lambda$. which is the effective degrees of freedom for the Lasso.

The glmnetfunction returns a sequence of models implied by lambda fitted by coordinate descent; our goal is to select the best. 
In this regard, we can use the cv.glmnet.This function does k-fold cross-validation for glmnet, produces a plot, and returns a value for the best lambda value.The function runs glmnet nfolds+1 times; the first to get the lambda sequence, and then the remainder to compute the fit with each of the folds omitted. The error is accumulated, and the average error and standard deviation over the folds is computed. THats how we obtain the best $\lambda$ according to the dataset.

Once we have the best lambda, we can use the coef() function  to obtain the coefficients.


```{r}
set.seed(2)
   CvLassoMod <- cv.glmnet(X, Y, alpha=1, nlambda=100,
                              lambda.min.ratio=0.0001)
best.lambda <- CvLassoMod$lambda.min
best.lambda 
coef(CvLassoMod, s = "lambda.min")
```
We see that some of the variable have been dropped. The Lasso regression penalty term, using the absolute value (rather than the square, as in the regression Ridge), forces some coefficients to be exactly equal to zero, if $\lambda$ is large enough. In practice, Lasso automatically performs a real selection of variables.
\color{red}\subsection{Ridge regression}
\color{black}
\color{magenta}\section{Cross Validation in order to choose the best model from a predictive point of view}
\color{black}
## note pour imane 
I am having doubt about this peut etre qu'on devrait aussi conclure si les modeles sont bon ou pas pour predie avant de choisir lequel est le meilleur?  

Cross-validation is the process of assessing how the results of a statistical analysis will generalize to an independent data set. If the model has been estimated over some, but not all, of the available data, then the model using the estimated parameters can be used to predict the held-back data. If, for example, the out-of-sample mean squared error, also known as the mean squared prediction error, is substantially higher than the in-sample mean square error, this is a sign of deficiency in the model.


or{magenta}\section{I.Study of the UScrime dataset}
\color{black}
As data Scientists we are asked to study the ability of regular linear models, lasso or ridge to predict the US crime rate dataset.
\color{red}\subsection{1.Preliminary work}
\color{black}
First of all we start with uploading the data and computing the correlation matrix to have a first sight on our dataset.

```{r,include=FALSE}
library(corrplot)
library(MASS)
```
\tiny
```{r,echo=FALSE}
data(UScrime)
corrplot(cor(UScrime), method="circle")
```
\normalsize
\newline The correlation plot shows us that our target variable is not correlated to all the covariables, hence a linear model containing all the 15 covariable may not be the best choice here.
We also notice that some of the co-variables are strongly correlated to each other (Po1 and GDP for exemple),thus they shouldn't be both on the model. 
So we are going to do  variable selection(forward,backward,stepwise) and then study the penalized regressions model (Lasso and ridge) and then decide which of those models are the best from a predictive point of view. 

\color{magenta}\section{Selection variables }
\color{red}\subsection{forward selection}
\color{black}
Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one. In each forward step, you add the one variable that gives the single best improvement to your model.
In the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters fit; at each step an add or drop will be performed that minimizes the AIC score.
\tiny
```{r,include=FALSE}
library(glmnet)
```
```{r,echo=FALSE}
crimereg=lm(y~.,data=UScrime)
regforward=step(lm(UScrime$y~1,data=UScrime),list(upper=crimereg),direction = 'forward')
```
The forward selection add successively Po1 , Ineq , Ed , M , Prob , U2
\color{red}\subsection{Backward selection}
\color{black}
Backward selection is a type of stepwise regression which begins  with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.
In the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters fit; at each step a drop will be performed that minimizes the AIC score.
```{r,echo=FALSE}
regbackward<-step(crimereg,direction = "backward")
```
There are 8 variables  that are selected. The backward selection drops successively So, time, lf, nw, po2, pop, gdp
\color{red}\subsection{Stepwise selection}
\color{black}
Stepwise regression is a combination of the forward and backward selection techniques. It is a modification of the forward selection so that after each step in which a variable was added, all candidate variables in the model are checked to see if their significance has been reduced below the specified tolerance level. If a nonsignificant variable is found, it is removed from the model.

```{r,echo=FALSE}
regboth<-step(crimereg,direction = 'both')

```
\color{magenta}\section{Penalized regression models }
By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process, variables are either retained or discarded. It often exhibits high variance, and so doesn't reduce the prediction error of the full model. Shrinkage methods are more continuous, and don't suffer as much from high variability.
\color{red}\subsection{Lasso regression}
\color{black}
Lasso regression find a parsimonious model which performs L1 regularization. The L1 regularization adds a penality equivalent to the absolute of the maginitude of regression coefficients and tries to minimize them 
Lasso translates each coefficient by a constant factor $\lambda$, truncating at zero. This is called "soft thresholding".
```{r,echo=FALSE}
library(glmnet)
X<- as.matrix(UScrime[ ,1:15])
Y <-as.matrix(UScrime$y)
LassoMod <- glmnet(X, Y, alpha=1, nlambda=100,lambda.min.ratio=0.0001)
```
Let us now see how the coefficient values change according to the lambda value. To do that, we will use the plot.glmnet()nction that produces a coefficient profile plot of the coefficient paths for a fitted glmnet obeject
```{r,echo=FALSE}
 plot(LassoMod,xvar="norm",label=TRUE)
```
In the previous figure, each curve corresponds to a variable. This plot shows the path of its coefficient against the L1-norm of the whole coefficient vector when $\lambda$ varying. The previous axis indicates the number of nonzero coefficients at the current $\lambda$. which is the effective degrees of freedom for the Lasso.

The glmnetfunction returns a sequence of models implied by lambda fitted by coordinate descent; our goal is to select the best. 
In this regard, we can use the cv.glmnet.This function does k-fold cross-validation for glmnet, produces a plot, and returns a value for the best lambda value.The function runs glmnet nfolds+1 times; the first to get the lambda sequence, and then the remainder to compute the fit with each of the folds omitted. The error is accumulated, and the average error and standard deviation over the folds is computed. THats how we obtain the best $\lambda$ according to the dataset.

Once we have the best lambda, we can use the coef() function  to obtain the coefficients.


```{r,echo=FALSE}
set.seed(2)
   CvLassoMod <- cv.glmnet(X, Y, alpha=1, nlambda=100,lambda.min.ratio=0.0001)
best.lambda <- CvLassoMod$lambda.min
best.lambda 
coef(CvLassoMod, s = "lambda.min")
```
We see that some of the variable have been dropped. The Lasso regression penalty term, using the absolute value (rather than the square, as in the regression Ridge), forces some coefficients to be exactly equal to zero, if $\lambda$ is large enough. In practice, Lasso automatically performs a real selection of variables.
