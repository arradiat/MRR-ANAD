---
title: "New york taxi trip duration"
author: "Mame Diarra Toure"
date: "2/28/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\section{Presentation}
We were challenged to build a model that predicts the total ride duration of taxi trips in New York City. Our primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.
The data set is divided in two file:  a training set composed of 1458644 observations of 11 initial variables with the target variable being the trip duration and a test data set composed of 625134 observations of 10 initial variables (the target variable was removed since it's the one we want to predict).
We are going to proceeds as follows:
first we will start by cleaning up our data set. That's mean getting ride of any observation that seem like an outlier.
Secondly we will start with getting acquainted with the data by looking at the different relation of the feature variables and the target vairable 
thirdly  we will add additional features that we think is relevant to our data set 
Finally we are going to compute various machine learning algorithms going from a simple linear model to more sophisticated algorithm.
\section{Loading of the training data and cleaning}
\subsection{Loading of the data}

```{r}
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation
library('alluvial') # visualisation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation
library('lubridate') # date and time
library('geosphere') # geospatial locations
library('leaflet') # maps
library('leaflet.extras') # maps
library('maps') # maps
library('xgboost') # modelling
library('caret') # modelling
library("glmnet")
```

```{r}
train <- as_tibble(fread("/Users/princessemame/APPRENTISSAGE AUTOMATIQUE /train.csv"))
```
\subsection{First Glimpse of the data}
```{r}
as.table(summary(train))
```




Thanks to the summary function  we have a first glimpse into our data set .
We can see for exemple that the longest ride last 3526282 seconds which corresponds to approximately 979 hours (that's definitely too long ) and the shortest ride last 1s (which is too short).
That means we will have to get rid of those extreme values.
\subsection{cleaning and modification of the data }
In this first cleaning we are going to get rid of extremes values 
We decide to remove all observation with a trip duration shorter than 1 mn and longer than 6 hours. We are also going to modify the datetime  format for a better manipulation
```{r}
train <- train[,-1]
```

```{r}
train <- subset(train,train$trip_duration<21600)
```
```{r}
train <-subset(train,train$trip_duration>=60)
```
```{r}
train <- train %>%
  mutate(pickup_datetime = ymd_hms(pickup_datetime),
         dropoff_datetime = ymd_hms(dropoff_datetime),
         vendor_id = factor(vendor_id),
         passenger_count = factor(passenger_count))
```

\subsection{Visualization of the different features and their relation with the target variable}
Visualisations of feature distributions and their relations are key to understanding a data set, and they often open up new lines of inquiry. Examinig the data in multiple forms enables us  to notice even subtle trends and correlations.
\subsubsection{distributions of each individual feature}
In this section we will begin by having a look at the distributions of the individual data features.

We start with a map of NYC and overlay a managable number of pickup coordinates to get a general overview of the locations and distances in question. For this visualisation we use the leaflet package, which includes a variety of cool tools for interactive maps. In this map you can zoom and pan through the pickup locations
```{r}
#set.seed(1234)
#foo <- sample_n(train, 8e3)

#leaflet(data = foo) %>% addProviderTiles("Esri.NatGeoWorldMap") %>%
# addCircleMarkers(~ pickup_longitude, ~pickup_latitude, radius = 1,
                   #color = "deeppink", fillOpacity = 0.5)
```

In turns out that almost all of our trips were in fact taking place in Manhattan only. Another notable hot-spot is JFK airport towards the south-east of the city.

The map gives us an idea what some of  our distributions could look like. Let’s start with plotting the target feature trip_duration:
```{r}
train %>%
  ggplot(aes((trip_duration))) +
  geom_histogram(fill = "pink", bins = 150)
train %>%
    ggplot(aes(log(trip_duration))) +
  geom_histogram(fill = "cornflowerblue", bins = 150)

```
Note the logarithmic x-axis and square-root y-axis.
We find:
the majority of rides follow a rather smooth distribution that looks almost log-normal with a peak just short of 1000 seconds, i.e. about 27 minutes.
In the plot above we can already see some daily and weekly modulations in the number of trips. Let’s investigate these variations.
```{r}
p1 <- train %>%
  group_by(passenger_count) %>%
  count() %>%
  ggplot(aes(passenger_count, n, fill = passenger_count)) +
  geom_col(fill=c("cornflowerblue","pink", "yellow", "orange","blue","deeppink","red","purple","brown")) +
  scale_y_sqrt() +
  theme(legend.position = "none")+ labs(x = "number of passengers", y = "Total number of pickups") 


p2 <- train %>%
  ggplot(aes(vendor_id, fill = vendor_id)) +
  geom_bar(fill=c("cornflowerblue","pink")) +
  theme(legend.position = "none")

p3 <- train %>%
  ggplot(aes(store_and_fwd_flag)) +
  geom_bar(fill=c("red","green")) +
  theme(legend.position = "none") +
  scale_y_log10()

p4 <- train %>%
  mutate(wday = wday(pickup_datetime
                     )) %>%
  group_by(wday, vendor_id) %>%
  count() %>%
  ggplot(aes(wday, n, colour = vendor_id)) +
  geom_point(size = 3) +
  labs(x = "Day of the week", y = "Total number of pickups") +
  theme(legend.position = "none")

p5 <- train %>%
  mutate(hpick = hour(pickup_datetime)) %>%
  group_by(hpick, vendor_id) %>%
  count() %>%
  ggplot(aes(hpick, n, color = vendor_id)) +
  geom_point(size = 3) +
  labs(x = "Hour of the day", y = "Total number of pickups") +
  theme(legend.position = "none")

```
```{r}
p1 
```
We can see that the majority of rides concern only one customers. There are few courses with 0 customers and 8 or 9 customers.
```{r}
p2
```
```{r}
p3
```


Here we can see the number of courses realised by each company. We can see that the vendor two realised approximately 200000 courses more than the vendor 1 
```{r}
p4
```
We notice that the distribution of the number of rides according to the week day is similar for the 2 vendors. The most busy day is friday with about 120000 rides for the vendor 2 and 105000  rides for the vendor 1. Monday is the calmest day with the least number of courses with around 85000 rides for the vendor 1 and 100000 courses for the vendor 2 
```{r}
p5
```

We can see here the number of pick ups according to the hour. Again the two vendors have very similar distribution . the busiest hour seems to be between 5  and 6 pm and the calmest around 5 am.
As one would intuitively expect, there is a strong dip during the early morning hours. We find another dip around 4pm and then the numbers increase towards the evening.
```{r}
p1 <- train %>%
  mutate(hpick = hour(pickup_datetime),
         Month = factor(month(pickup_datetime, label = TRUE))) %>%
  group_by(hpick, Month) %>%
  count() %>%
  ggplot(aes(hpick, n, color = Month)) +
  geom_line(size = 1.5) +
  labs(x = "Hour of the day", y = "count")

p2 <- train %>%
  mutate(hpick = hour(pickup_datetime),
         wday = factor(wday(pickup_datetime, label = TRUE, week_start = 1))) %>%
  group_by(hpick, wday) %>%
  count() %>%
  ggplot(aes(hpick, n, color = wday)) +
  geom_line(size = 1.5) +
  labs(x = "Hour of the day", y = "count")

```

```{r}
p1
```
We find that January and June have fewer trips, whereas March and April are busier months. This tendency is observed for both vendor_ids.We can also notice that the distribution of trips for each month are very similar.

```{r}
p2
```
Here the distribution of the number of trips accroding to the hour of the day is quite different  for each day.
The weekend (Sat and Sun, plus Fri to an extend) have higher trip numbers during the early morning ours but lower ones in the morning between 5 and 10, which can most likely be attributed to the contrast between NYC business days and weekend night life. In addition, trip numbers drop on a Sunday evening/night.


```{r}
p1 <- train %>%
  filter(pickup_longitude > -74.05 & pickup_longitude < -73.7) %>%
  ggplot(aes(pickup_longitude)) +
  geom_histogram(fill = "cornflowerblue", bins = 40)

p2 <- train %>%
  filter(dropoff_longitude > -74.05 & dropoff_longitude < -73.7) %>%
  ggplot(aes(dropoff_longitude)) +
  geom_histogram(fill = "pink", bins = 40)

p3 <- train %>%
  filter(pickup_latitude > 40.6 & pickup_latitude < 40.9) %>%
  ggplot(aes(pickup_latitude)) +
  geom_histogram(fill = "cornflowerblue", bins = 40)

p4 <- train %>%
  filter(dropoff_latitude > 40.6 & dropoff_latitude < 40.9) %>%
  ggplot(aes(dropoff_latitude)) +
  geom_histogram(fill = "pink", bins = 40)

```
```{r}
p1
```
```{r}
p2
```
```{r}
p3
```


```{r}
p4
```
Here we had constrain the range of latitude and longitude values, because there are a few cases which are way outside the NYC boundaries. The resulting distributions are consistent with the focus on Manhattan that we had already seen on the map. 


\subsection{visualisation of the Relation between the arget and the features}
While the previous section looked primarily at the distributions of the individual features, here we will examine in more detail how those features are related to each other and to our target trip_duration.

\subsubsection{Pickup date/time vs trip_duration}
```{r}
p1 <- train %>%
  mutate(wday = wday(pickup_datetime, label = TRUE, week_start = 1)) %>%
  group_by(wday, vendor_id) %>%
  summarise(mean_duration = mean(trip_duration)) %>%
  ggplot(aes(wday, mean_duration, color = vendor_id)) +
  geom_point(size = 4) +
  labs(x = "Day of the week", y = "Mean trip duration [s]")

p2 <- train %>%
  mutate(hpick = hour(pickup_datetime)) %>%
  group_by(hpick, vendor_id) %>%
  summarise(mean_duration = mean(trip_duration)) %>%
  ggplot(aes(hpick, mean_duration, color = vendor_id)) +
  geom_point(size = 4) +
  labs(x = "Hour of the day", y = "Mean trip duration [s]") +
  theme(legend.position = "none")

```
```{r}
p1
```
Previously we've seen that thurdays were the second busiest day for both vendors  they also have  longer trips in average. Mondays were the calmest days however they dont have the fastest trips in average. This could mean that the calmer day doesn't lead to faster trip or in the opposite busiest days doest not necessarily mean longer trips. We also notice that the vendor 2 which have the more frequent rips has also the longest trip duration .

```{r}
p2
```
Over the course of a typical day we find a peak in the early afternoon and dips around 5-6am and 8pm. 
According to these two plots we can conclude that 
The weekday and hour of a trip appear to be important features for predicting its duration and should be included in a successful model.
Also since the vendor 2 has higher trip duration.  It will be worth adding the vendor_id feature to a model to test its predictive importance.
 \subsubsubsection{Passenger count and Vendor-id vs trip_duration}
The next question we are asking is whether different numbers of passengers and/or the different vendors are correlated with the duration of the trip. We choose to examine this issue using a series of boxplots for the passenger_counts together with a facet wrap which contrasts the two vendor_ids:
```{r}
train %>%
  ggplot(aes(passenger_count, trip_duration, color = passenger_count)) +
  geom_boxplot() +
  scale_y_log10() +
  theme(legend.position = "none") +
  facet_wrap(~ vendor_id) +
  labs(y = "Trip duration [s]", x = "Number of passengers")

```
First of all. we notice that only  the vendor 2 realise course woth more than 6 passengers.
Between 1 and 6 passengers the median trip durations are remarkably similar, in particular for vendor 2. There might be differences for vendor 1, but they are small (note the logarithmic y-axis)
Conclusion:
The number of passengers doesn't seem to have a big  effect on the trip duration they can be kept ou of a the model
```{r}
train %>%
  ggplot(aes(log(trip_duration), fill = vendor_id)) +
  geom_density(position = "stack") 

```
the trip duration seems to follow a log normla distribution 
\subsubsection{ Store and Forward vs trip_duration}
```{r}
train %>%
  filter(vendor_id == 1) %>%
  ggplot(aes(passenger_count, trip_duration, color = passenger_count)) +
  geom_boxplot() +
  scale_y_log10() +
  facet_wrap(~ store_and_fwd_flag) +
  theme(legend.position = "none") +
  labs(y = "Trip duration [s]", x = "Number of passengers") +
  ggtitle("Store_and_fwd_flag impact")
```
We find that there is no overwhelming differences between the stored and non-stored trips. The stored ones might be slightly longer, though, and don’t include any of the suspiciously long trips.
\section{Adding relevant Feature: the direct distance }
The trip duration is very likely to be correlated with the distance betwenn the pick up and drop off location so we are goin to add a feature corresponding to the distance using the geolocalisation cooridnates and the R package geosphere
```{r}
library(geosphere)
n <- nrow(train)
latA <-as.matrix(train[,6])
lonA <- as.matrix(train[,5])
latB <- as.matrix(train[,8])
lonB <- as.matrix(train[,7])
p1 <- matrix(data = c(lonA,latA),nrow=n, ncol=2)
p2<- matrix(data = c(lonB,latB),nrow=n, ncol=2)
distance <- distGeo(p1,p2)
distance <- log(distance+1)
train <- cbind(train,distance)
# switch des colonnes distance et duree
a <- c(colnames(train))
b <- a
b[11] <- a[12]
b[12 ] <- a[11]

```

\subsection{second cleaning : getting rid of extreme distance}
We are going to get rid off trips further than 100 km
```{r}
train <- subset(train,train$distance<=100000)
train <- subset( train, train$distance>0)

set.seed(4321)
train %>%
  sample_n(5e4) %>%
  ggplot(aes(distance, log(trip_duration)) )+
  geom_point()+

  labs(x = "Direct distance [m]", y = "Trip duration [s]")
  
```
train <- train[,-1]

it seems like the trip duration increases with the distant which is understandable
Should i add external data 

```{r}
set.seed(123)
train_coord_pickup <- cbind(train$pickup_latitude, train$pickup_longitude)
# Applying the Kmeans algorithm in order to create 'neighborhoods'
nb_pickup <- kmeans(train_coord_pickup, centers=3, algorithm='Lloyd', iter.max=500 )
 
set.seed(123)
train_coord_dropoff <- cbind(train$dropoff_latitude, train$dropoff_longitude)
# Applying the Kmeans algorithm in order to create 'neighborhods'
nb_dropoff <- kmeans(train_coord_dropoff, centers=3, algorithm='Lloyd', iter.max=500 )
 
```

```{r}
#p1=ggplot(data.frame(train_coord_pickup), aes(X2,X1,col =as.factor(nb_pickup$cluster))) + 
 #geom_point(fill = "grey50") + coord_fixed(1.3)  
```

```{r}
#p1
```

\section{model}
```{r}
train <- train %>%
  mutate(month = month(pickup_datetime, label = TRUE),
         wday = wday(pickup_datetime, label = TRUE, week_start = 1),
         wday = fct_relevel(wday, c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")),
         hour=hour(pickup_datetime),
         trip_duration=log(trip_duration)
          )
         
```

```{r}
subset(train, train$passenger_count<7)
```

```{r}
#j'neleve datetime parce que j'ai deja recuper le mois le jour et l'heure dans 3 autres colonnes 

#j'enleve store and flag pour la meme raiosn aussi
train <- train[,-c(2, 3, 9)]

```

```{r}
train$passenger_count <- as.numeric(train$passenger_count)-1
```
```{r}
train$wday <- as.numeric(train$wday)
```
```{r}
train$month <- as.numeric(train$month)
```
```{r}
corrplot(cor(data.matrix(train)))
```


```{r}
modreg_R1 <- lm(train$trip_duration~.,data=train)
```

```{r}
summary(modreg_R1)
```
```{r}
intercept <- rep(1,nrow(train))
X=as.matrix(cbind(intercept,train[,c(1:6,8:11)]))
beta=t(t(modreg_R1$coefficients))
```



```{r}
test <- as_tibble(fread("/Users/princessemame/APPRENTISSAGE AUTOMATIQUE /test.csv"))
```
```{r}
summary( test)
```
```{r}
submit<- as_tibble(fread("/Users/princessemame/APPRENTISSAGE AUTOMATIQUE /sample_submission.csv"))
```
```{r}
test <- test[,-1]
```

```{r}
test <- test %>%
  mutate(pickup_datetime = ymd_hms(pickup_datetime))
```
```{r}
summary(test)
```
```{r}
library(geosphere)
n <- nrow(test)
latA <-as.matrix(test$pickup_latitude)
lonA <- as.matrix(test$pickup_longitude)
latB <- as.matrix(test$dropoff_latitude)
lonB <- as.matrix(test$dropoff_longitude)
p1 <- matrix(data = c(lonA,latA),nrow=n, ncol=2)
p2<- matrix(data = c(lonB,latB),nrow=n, ncol=2)
distance <- distGeo(p1,p2)
distance <- log(distance+1)
test <- cbind(test,distance)

```
```{r}
test <- test%>%
  mutate(store_and_fwd_flag = as.integer(factor(store_and_fwd_flag)),
         vendor_id = as.integer(vendor_id),
         month = (month(pickup_datetime)),
         hour = hour(pickup_datetime),
         wday = wday(pickup_datetime, label = TRUE, abbr = TRUE) , wday = as.integer(fct_relevel(wday, c("Sun", "Sat", "Mon", "Tue", "Wed", "Thu", "Fri"))),
)
  
```
```{r}
test$wday <- as.numeric(test$wday)
```
```{r}
test$month <- as.numeric(test$month)
```
```{r}
test <- test[,-c(2, 8)]
```
```{r}
summary(beta)
```
```{r}
#Feature Selection
#regfor <- step(lm(train$trip_duration~1,data=train),list(upper=modreg_R1),direction = "forward")
regboth <- step(lm(train$trip_duration~1,data=train),list(upper=modreg_R1),direction = "both")

```
```{r}
regbackward <- step(modreg_R1,direction = "backward")
```

```{r}
X=as.matrix(cbind(rep(1,(nrow(test))), test))
beta=t(t(modreg_R1$coefficients))
Y_hat=X%*%beta

```

```{r}
submit$trip_duration <- exp(Y_hat)
summary(submit)
```
```{r}
#subset(submit,submit$trip_duration<0)


submit[which(submit$trip_duration<0),2] <- mean(submit$trip_duration)
```
#model1
```{r}
write.csv(x = submit, file = "sample_submission3.csv",row.names = FALSE)
```

#LASSO
```{r}
library(glmnet)
```
```{r}
modreg1=cv.glmnet(data.matrix(train[,c(1:7,9:11)]), (train$trip_duration), alpha=1)
```



```{r}
best_lam <- modreg1$lambda.min
```


```{r}
lasso_best <- glmnet(as.matrix(train[,c(1:7,9:11)]), train$trip_duration, alpha = 1, lambda = best_lam)
Y_test<- predict(lasso_best, s = best_lam, newx = as.matrix(test))
```
```{r}
submit$trip_duration <- Y_test
```
```{r}
submit[which(submit$trip_duration<0),2] <- mean(submit$trip_duration)
```
```{r}
summary(submit)
```
```{r}
write.csv(x = submit, file = "sample_submission.csv",row.names = FALSE)
```

#RIDGE
```{r}
library(glmnet)
CvRidgeMod=cv.glmnet(data.matrix(train[,c(1:7,9:11)]), train$trip_duration,alpha=0,nlambda=100,lambda.min.ratio=0.0001)
best.lam=CvRidgeMod$lambda.min 
ridge_best <- glmnet(as.matrix(train[,c(1:7,9:11)]), train$trip_duration, alpha = 0, lambda = best_lam)
Y_test<- predict(ridge_best, s = best_lam, newx = as.matrix(test))

```

```{r}
submit$trip_duration <- Y_test
```
```{r}
submit[which(submit$trip_duration<0),2] <- mean(submit$trip_duration)
```
```{r}
summary(submit)
```
```{r}
write.csv(x = submit, file = "sample_submission.csv",row.names = FALSE)
```
#KNN
```{r}
prediction <- FNN::knn.reg(data.matrix(train[,-8]),test,log(train$trip_duration), k = 100, algorithm="kd_tree")
```


```{r}
Y_KNN <- exp(prediction$pred)
submit$trip_duration <- Y_KNN
```

```{r}
write.csv(x = submit, file = "submitKNN.csv",row.names = FALSE)
```

```{r}
data2 <- sample_n(train,1000)
```

```{r,warning=FALSE}
RMSE1=c()
RMSE_lm=c()
residu_lm=matrix(nrow =200, ncol=20)
for (i in 1:20){
  smp_size = floor(0.8 * nrow(data2))
  set.seed(10+200*i)
  train_ind = sample(seq_len(nrow(data2)),size = smp_size) 
  TabTrain =data2[train_ind,] 
  TabTest=data2[-train_ind,]
  modreg1= lm(TabTrain$trip_duration~.,data=TabTrain)
  Y_test=predict(modreg1,newdata=TabTest[,-7],interval="confidence")
  Y_predicted=data.frame(Y_test)$fit
  Y_reel=TabTest$trip_duration
  residu_lm[,i]=Y_reel-Y_predicted
  RMSE_lm[i]=sqrt(((sum(residu_lm[,i])**2))/length(TabTest))
}
```


```{r,warning=FALSE}
RMSE_ridge=c()
residu_ridge=matrix(nrow =200, ncol=20)
for (i in 1:20){
  smp_size = floor(0.8 * nrow(data2))
  set.seed(200*i+10)
  train_ind = sample(seq_len(nrow(data2)),size = smp_size) 
  
  TabTrain =data2[train_ind,] 
  TabTest=data2[-train_ind,]
  
  modreg1=cv.glmnet(data.matrix(TabTrain[,c(1:6,8:11)]),TabTrain$trip_duration,alpha=0,nlambda=100,lambda.min.ratio=0.0001)
  
  best_lambda=modreg1$lambda.min
  
  best_ridge=glmnet(data.matrix(TabTrain[,-7]),TabTrain$trip_duration ,lambda=best_lambda)
  
  pred=predict(best_ridge, s = best_lambda, newx= data.matrix(TabTest[,c(1:6,8:11)]))
  Y_predicted=data.matrix(pred)
  Y_reel=TabTest$trip_duration
  residu_ridge[,i]=Y_reel-t(Y_predicted)
  RMSE_ridge[i]=sqrt(((sum(residu_ridge[,i])**2))/length(TabTest))
}
```


```{r,warning=FALSE}
RMSE_lasso=c()
residu_lasso=matrix(nrow =200, ncol=20)
for (i in 1:20){
  lambdaseq <- seq(0,50,0.01)
  smp_size = floor(0.8 * nrow(data2))
  set.seed(200*i+10)
  train_ind = sample(seq_len(nrow(data2)),size = smp_size) 
  TabTrain =data2[train_ind,] 
  TabTest=data2[-train_ind,]
  modreg1=cv.glmnet(data.matrix(TabTrain[,-7]), TabTrain$trip_duration, alpha=1, lambda=lambdaseq)
  best_lam <- modreg1$lambda.min
  lasso_best <- glmnet(data.matrix(TabTrain[,-7]), TabTrain$trip_duration, alpha = 1, lambda = best_lam)
  Y_test<- predict(lasso_best, s = best_lam, newx = data.matrix(TabTest[,c(1:6,8:11)]))
  Y_predicted=data.frame(Y_test)
  Y_reel=TabTest$trip_duration
  residu_lasso[,i]=(Y_reel)-t(Y_predicted)
  RMSE_lasso[i]=sqrt(((sum(residu_lasso[,i])**2))/length(TabTest))
}
RMSE_EN=c()
residu_EN=matrix(nrow =200, ncol=20)
for (i in 1:20){
  lambdaseq <- seq(0,50,0.01)
  smp_size = floor(0.8 * nrow(data2))
  set.seed(200*i+10)
  train_ind = sample(seq_len(nrow(data2)),size = smp_size) 
  TabTrain =data2[train_ind,] 
  TabTest=data2[-train_ind,]
  modreg1=cv.glmnet(data.matrix(TabTrain[,-7]), TabTrain$trip_duration ,alpha=0.5,nlambda=100,lambda.min.ratio=0.0001)
  best_lam <- modreg1$lambda.min
  EN_best <- glmnet(data.matrix(TabTrain[,c(1:6,8:11)]), TabTrain$trip_duration,alpha=0.5,  lambda = best_lam)
  Y_test<- predict(EN_best, s = best_lam, newx = data.matrix(TabTest[,c(1:6,8:11)]))
  Y_predicted=data.frame(Y_test)
  Y_reel=TabTest$trip_duration
  residu_EN[,i]=(Y_reel)-t(Y_predicted)
  RMSE_EN[i]=sqrt(((sum(residu_EN[,i])**2))/length(TabTest))
}
```
```{r}
library(FNN)
RMSE1=c()
RMSE_KNN=c()
residu_KNN=matrix(nrow =200, ncol=20)
for (i in 1:20){
  smp_size = floor(0.8 * nrow(data2))
  set.seed(10+200*i)
  train_ind = sample(seq_len(nrow(data2)),size = smp_size) 
  TabTrain =data2[train_ind,] 
  TabTest=data2[-train_ind,]
  prediction <- FNN::knn.reg(data.matrix(TabTrain[,c(1:6,8:11)]),data.matrix(TabTest[,-7]), TabTrain$trip_duration, k = 25, algorithm="kd_tree")  
  Y_predicted <-data.matrix(prediction$pred)
  Y_reel=TabTest$trip_duration
  
  residu_KNN[,i]=Y_reel-Y_predicted
  RMSE_KNN[i]=sqrt((((sum(residu_KNN[,i])**2)))/length(TabTest))
}
```


```{r}
group=c("lm","Lasso","Ridge","ElasticNet","KNN")
boxplot(RMSE_lm,RMSE_lasso,RMSE_ridge,RMSE_EN,RMSE_KNN,names=group,notch=F, outlier.color = "red", outlier.shape = 8, outlier.size = 4, col=(c("darkgreen","purple","pink","blue","red")))
grid()
```

